{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sreenath\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class   \n",
      "0           0      3            0                   0        3      2  \\\n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Sreenath/Desktop/counterspeech/labeled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sreenath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Sreenath/Desktop/counterspeech/labeled_data.csv\")\n",
    "\n",
    "# Remove URLs and emojis\n",
    "def remove_urls_emojis(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_urls_emojis)\n",
    "\n",
    "# Convert to lowercase\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Handle contractions (if necessary)\n",
    "# import contractions\n",
    "# df['tweet'] = df['tweet'].apply(contractions.fix)\n",
    "\n",
    "# ... Add more cleaning steps based on your specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls_emojis(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove emojis and other symbols\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_urls_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and reduced dataset saved to cleaned_labeled_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sreenath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "\n",
    "# Cleaning function to remove URLs and emojis\n",
    "def remove_urls_emojis(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove emojis and other symbols\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['tweet'] = df['tweet'].apply(remove_urls_emojis)\n",
    "\n",
    "# Drop rows with missing tweet text\n",
    "df = df.dropna(subset=['tweet'])\n",
    "\n",
    "# Load stop words and remove them from the tweets\n",
    "nltk.download('stopwords')  # Download stopwords if not already available\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "\n",
    "# Randomly sample 2000 tweets from the cleaned dataset\n",
    "df_sampled = df.sample(n=2000, random_state=42)\n",
    "\n",
    "# Save the cleaned and sampled dataset to the same file name provided by you\n",
    "cleaned_data_file = \"cleaned_labeled_data.csv\"\n",
    "df_sampled.to_csv(cleaned_data_file, index=False)\n",
    "\n",
    "print(f\"Cleaned and reduced dataset saved to {cleaned_data_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('C:/Users/Sreenath/Desktop/counterspeech/cleaned_labeled_data.csv')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.5, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  count  hate_speech  offensive_language  neither  class   \n",
      "0           2326      3            0                   3        0      1  \\\n",
      "1          16283      3            0                   3        0      1   \n",
      "2          19362      3            0                   1        2      2   \n",
      "3          16780      3            0                   3        0      1   \n",
      "4          13654      3            1                   2        0      1   \n",
      "...          ...    ...          ...                 ...      ...    ...   \n",
      "1995       10888      3            2                   1        0      0   \n",
      "1996       15054      3            0                   3        0      1   \n",
      "1997       18725      3            0                   3        0      1   \n",
      "1998        7146      3            1                   2        0      1   \n",
      "1999       16496      6            0                   5        1      1   \n",
      "\n",
      "                                                  tweet  \n",
      "0                     934 8616 got missed call yo bitch  \n",
      "1     RT KINGTUNCHI Fucking bad bitch gone need mone...  \n",
      "2     RT eanahS 1inkkofrosess lol credit aint near g...  \n",
      "3     RT MaxinBetha Wipe cum faggot RT 80sbaby4life ...  \n",
      "4     Niggas cheat bitch dont expect pay back WHATSO...  \n",
      "...                                                 ...  \n",
      "1995  want punch throat get order wrong stupid bitch...  \n",
      "1996  RT Chykalet youre fucking sons barber still go...  \n",
      "1997             RT MindAtEase Fuck twerking bitch cook  \n",
      "1998  ryanhoover toddkincannon Im vet Ryan TK rocks ...  \n",
      "1999  RT LaLeaa wcw babes undrgrundghosts amp 0beyyo...  \n",
      "\n",
      "[2000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_labeled_data.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Loss = 0.0018991599574126833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaRegressionHead(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (regressor): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:/Users/Sreenath/Desktop/counterspeech/cleaned_labeled_data.csv')\n",
    "\n",
    "# Create dummy float labels (e.g., assign all tweets a float value '0.0')\n",
    "df['label'] = 0.0  # Dummy float label; replace this with actual float label logic if needed\n",
    "\n",
    "# Splitting the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')  # Using base RoBERTa model\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Convert the labels to PyTorch tensors with float type\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)  # Float labels for regression\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch DataLoader for training data\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define a regression head on top of RoBERTa\n",
    "class RobertaRegressionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaRegressionHead, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)  # Single output for regression\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token's representation\n",
    "        regression_output = self.regressor(hidden_state)\n",
    "        return regression_output\n",
    "\n",
    "model = RobertaRegressionHead()\n",
    "\n",
    "# Set up optimizer, loss function (MSE for regression), and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error for regression\n",
    "epochs = 1\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs = outputs.squeeze()  # Since we have a single regression output per sample\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}: Loss = {total_loss / len(train_loader)}')\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'C:/Users/Sreenath/Desktop/counterspeech/roberta_regression'\n",
    "model.roberta.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Load the trained model for inference\n",
    "loaded_model = RobertaRegressionHead()\n",
    "loaded_model.roberta.from_pretrained(model_save_path)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets trained: 1400\n"
     ]
    }
   ],
   "source": [
    "# ... rest of your code ...\n",
    "\n",
    "print(f\"Number of tweets trained: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/Sreenath/Desktop/counterspeech/trained_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaForCausalLM, RobertaTokenizer, pipeline\n",
    "\n",
    "# Step 1: Load Your Dataset\n",
    "df = pd.read_csv('C:/Users/Sreenath/Desktop/counterspeech/trained_data.csv')\n",
    "\n",
    "# Step 2: Preprocess the Data (if necessary)\n",
    "# Assuming the tweets are in a column named 'tweet'\n",
    "tweets = df['tweet'].tolist()\n",
    "\n",
    "# Step 3: Load a Pre-trained RoBERTa Model for Causal Language Modeling\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForCausalLM.from_pretrained('roberta-base')  # Use a model with causal LM head\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 4: Generate Counter-Speech\n",
    "counter_speech = []\n",
    "for tweet in tweets:\n",
    "    generated = generator(tweet, max_new_tokens=50, num_return_sequences=1)  # Generate 50 new tokens\n",
    "    counter_speech.append(generated[0]['generated_text'])\n",
    "\n",
    "# Step 5: Save the Results\n",
    "df['counter_speech'] = counter_speech\n",
    "df.to_csv('C:/Users/Sreenath/Desktop/counterspeech/hate_speech_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS: Your input prompt here.\n",
      "Predictions: [0.14271286 0.12606551]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the trained dataset\n",
    "dataset_path = 'C:/Users/Sreenath/Desktop/counterspeech/trained_data.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = 'roberta-base'  # Replace with your actual model name if different\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define the input prompt\n",
    "prompt_text = \"Your input prompt here.\"  # Replace with your actual prompt\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# Generation parameters\n",
    "args = {\n",
    "    'length': 50,  # Maximum number of tokens to generate\n",
    "    'temperature': 0.7,  # Controls randomness\n",
    "    'k': 50,  # Limits the next possible words to the top-k options\n",
    "    'p': 0.95,  # Nucleus sampling parameter\n",
    "    'repetition_penalty': 1.2,  # Penalizes repetition\n",
    "    'num_return_sequences': 1,  # Number of sequences to generate\n",
    "}\n",
    "\n",
    "# If you are performing regression, you'll typically want to use the model's forward method\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits  # Get the logits from the model output\n",
    "\n",
    "# Process the logits according to your regression task\n",
    "predictions = logits.squeeze().numpy()  # Convert to numpy array for further processing\n",
    "\n",
    "# Output the predictions\n",
    "print(\"HS:\", prompt_text)  # HS stands for \"Human Speech\" or input prompt\n",
    "print(\"Predictions:\", predictions)  # Display the regression predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: C:/Users/Sreenath/Desktop/counterspeech/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the trained dataset\n",
    "dataset_path = 'C:/Users/Sreenath/Desktop/counterspeech/trained_data.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = 'roberta-base'  # Replace with your actual model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Prepare a list to hold predictions\n",
    "predictions_list = []\n",
    "\n",
    "# Iterate through each prompt in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    prompt_text = row['tweet']  # Replace 'your_prompt_column' with the actual column name\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits  # Get the logits from the model output\n",
    "\n",
    "    # Process the logits for regression task (or classification if needed)\n",
    "    predictions = logits.squeeze().numpy()  # Convert to numpy array\n",
    "\n",
    "    # Append the results to the predictions list\n",
    "    predictions_list.append({\n",
    "        'Prompt': prompt_text,\n",
    "        'Prediction': predictions.tolist()  # Convert numpy array to list for CSV compatibility\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the predictions list\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "# Save the predictions to a new CSV file\n",
    "output_path = 'C:/Users/Sreenath/Desktop/counterspeech/predictions.csv'  # Adjust path as needed\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Predictions saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter predictions saved to: C:/Users/Sreenath/Desktop/counterspeech/counter_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "\n",
    "# Load the trained dataset\n",
    "dataset_path = 'C:/Users/Sreenath/Desktop/counterspeech/trained_data.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the classifier model\n",
    "classifier_model_name = 'roberta-base'  # Your classification model\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(classifier_model_name)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(classifier_model_name)\n",
    "\n",
    "# Load the text generation model\n",
    "generator_model_name = 'gpt2'  # Change to your text generation model if needed\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name)\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(generator_model_name)\n",
    "\n",
    "# Prepare a list to hold results\n",
    "results_list = []\n",
    "\n",
    "# Define a threshold for negative sentiment (example value)\n",
    "threshold = 0.0  # Adjust based on your needs\n",
    "\n",
    "# Iterate through each prompt in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    prompt_text = row['tweet']  # Replace with actual column name\n",
    "    input_ids = classifier_tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier_model(input_ids=input_ids)\n",
    "        logits = outputs.logits.squeeze().numpy()  # Get logits\n",
    "\n",
    "    # Interpret logits (example: for regression)\n",
    "    prediction = logits[0]  # Assuming single output for simplicity\n",
    "\n",
    "    # Generate counter text if needed\n",
    "    if prediction < threshold:  # Adjust condition as necessary\n",
    "        # Generate counter text using the generator model\n",
    "        gen_input_ids = generator_tokenizer.encode(f\"Counter response to: {prompt_text}\", return_tensors='pt')\n",
    "        output_sequences = generator_model.generate(\n",
    "            input_ids=gen_input_ids,\n",
    "            max_length=50,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        counter_text = generator_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        counter_text = \"No counter response needed.\"\n",
    "\n",
    "    # Store the results\n",
    "    results_list.append({\n",
    "        'Prompt': prompt_text,\n",
    "        'Prediction': prediction,\n",
    "        'Counter Text': counter_text\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_path = 'C:/Users/Sreenath/Desktop/counterspeech/counter_predictions.csv'  # Adjust path as needed\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Counter predictions saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sreenath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses generated and saved to 'generated_responses.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the trained dataset\n",
    "dataset_path = \"C:/Users/Sreenath/Desktop/counterspeech/hate_speech_data.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the pre-trained GPT model and tokenizer\n",
    "model_name = \"gpt2\"  # You can choose a larger model if needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set pad_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to be the same as the end of sentence token\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Create an attention mask\n",
    "    attention_mask = torch.ones(inputs.shape, device=inputs.device)  # all ones for attention mask\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,  # Generate a specified number of new tokens\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# List to hold the generated responses\n",
    "responses = []\n",
    "\n",
    "# Generate responses for each entry in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    prompt = row['counter_speech']  # Replace with the name of the column you want to use for prompts\n",
    "    response = generate_response(prompt)\n",
    "    responses.append(response)\n",
    "\n",
    "# Create a new DataFrame to store the responses\n",
    "response_df = pd.DataFrame({'prompt': data['counter_speech'], 'response': responses})  # Make sure to use the correct column name\n",
    "\n",
    "# Save the responses to a new CSV file\n",
    "response_df.to_csv(\"C:/Users/Sreenath/Desktop/counterspeech/generated_responses.csv\", index=False)\n",
    "\n",
    "print(\"Responses generated and saved to 'generated_responses.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses generated so far: 2000 out of 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Responses generated so far: {index + 1} out of {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create a CSV writer\n",
    "with open(\"C:/Users/Sreenath/Desktop/counterspeech/generated_responses.csv\", \"w\", newline=\"\" , encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = ['prompt', 'response']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    for index, row in response_df.iterrows():\n",
    "        writer.writerow({'prompt': row['prompt'], 'response': row['response']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def separate_text(text):\n",
    "    \"\"\"Separates English text and generated text from the given text.\"\"\"\n",
    "    # Define regular expressions for English and generated text patterns\n",
    "    english_pattern = r\"[A-Za-z\\s]+\"\n",
    "    generated_pattern = r\"[^\\s]+\"  # Matches any non-whitespace characters\n",
    "\n",
    "    # Find matches for English and generated text\n",
    "    english_matches = re.findall(english_pattern, text)\n",
    "    generated_matches = re.findall(generated_pattern, text)\n",
    "\n",
    "    # Join the matches into separate strings\n",
    "    english_text = \" \".join(english_matches)\n",
    "    generated_text = \" \".join(generated_matches)\n",
    "\n",
    "    return english_text, generated_text\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"C:/Users/Sreenath/Desktop/counterspeech/generated_responses.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Separate English and generated text for each response\n",
    "english_texts = data['response'].apply(lambda x: separate_text(x)[0])\n",
    "generated_texts = data['response'].apply(lambda x: separate_text(x)[1])\n",
    "\n",
    "# Create a new DataFrame with only the new fields\n",
    "new_data = pd.DataFrame({'english_text': english_texts, 'generated_text': generated_texts})\n",
    "\n",
    "# Create a new file name\n",
    "new_file_path = \"C:/Users/Sreenath/Desktop/counterspeech/separated_responses.csv\"\n",
    "\n",
    "# Save the changes to the new file\n",
    "new_data.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
